<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama Labs on My New Hugo Site</title><link>https://example.org/</link><description>Recent content in Ollama Labs on My New Hugo Site</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://example.org/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting Started</title><link>https://example.org/docs/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/docs/getting-started/</guid><description>&lt;h1 id="getting-started-with-ollama">Getting Started with Ollama&lt;/h1>
&lt;p>This guide will help you install Ollama and run your first language model locally.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>Ollama is available for macOS, Linux, and Windows.&lt;/p>
&lt;h3 id="macos">macOS&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install ollama
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="linux">Linux&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -fsSL https://ollama.ai/install.sh | sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="windows">Windows&lt;/h3>
&lt;p>Windows support is available through WSL (Windows Subsystem for Linux).&lt;/p>
&lt;h2 id="running-your-first-model">Running Your First Model&lt;/h2>
&lt;p>Once you have Ollama installed, you can run your first model with a simple command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ollama run llama2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will download the Llama 2 model (if you don&amp;rsquo;t already have it) and start a chat session.&lt;/p></description></item><item><title>Lab 1: Running Your First Model</title><link>https://example.org/labs/lab1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/labs/lab1/</guid><description>&lt;h1 id="lab-1-running-your-first-model-with-ollama">Lab 1: Running Your First Model with Ollama&lt;/h1>
&lt;p>In this lab, you&amp;rsquo;ll learn how to run your first language model using Ollama and interact with it through the command line.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Ollama installed on your system&lt;/li>
&lt;li>Terminal or command prompt access&lt;/li>
&lt;li>At least 5GB of free disk space&lt;/li>
&lt;/ul>
&lt;h2 id="step-1-verify-ollama-installation">Step 1: Verify Ollama Installation&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ollama --version
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="step-2-run-your-first-model">Step 2: Run Your First Model&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ollama run llama2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will download the model if needed and start an interactive chat session.&lt;/p></description></item><item><title>Lab 2: Python Integration</title><link>https://example.org/labs/lab2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/labs/lab2/</guid><description>&lt;h1 id="lab-2-integrating-ollama-with-python">Lab 2: Integrating Ollama with Python&lt;/h1>
&lt;p>In this lab, you&amp;rsquo;ll learn how to use Ollama from Python applications.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Ollama installed and working&lt;/li>
&lt;li>Python 3.8 or newer&lt;/li>
&lt;li>Basic Python knowledge&lt;/li>
&lt;/ul>
&lt;h2 id="example-python-code">Example Python Code&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> requests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">generate_response&lt;/span>(prompt, model&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;llama2&amp;#34;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;http://localhost:11434/api/generate&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data &lt;span style="color:#f92672">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;model&amp;#34;&lt;/span>: model,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;prompt&amp;#34;&lt;/span>: prompt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> response &lt;span style="color:#f92672">=&lt;/span> requests&lt;span style="color:#f92672">.&lt;/span>post(url, json&lt;span style="color:#f92672">=&lt;/span>data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> response&lt;span style="color:#f92672">.&lt;/span>status_code &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">200&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> response_text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> line &lt;span style="color:#f92672">in&lt;/span> response&lt;span style="color:#f92672">.&lt;/span>text&lt;span style="color:#f92672">.&lt;/span>strip()&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">try&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> response_json &lt;span style="color:#f92672">=&lt;/span> json&lt;span style="color:#f92672">.&lt;/span>loads(line)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#e6db74">&amp;#39;response&amp;#39;&lt;/span> &lt;span style="color:#f92672">in&lt;/span> response_json:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> response_text &lt;span style="color:#f92672">+=&lt;/span> response_json[&lt;span style="color:#e6db74">&amp;#39;response&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">except&lt;/span> json&lt;span style="color:#f92672">.&lt;/span>JSONDecodeError:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> response_text
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Error: &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>response&lt;span style="color:#f92672">.&lt;/span>status_code&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74"> - &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>response&lt;span style="color:#f92672">.&lt;/span>text&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Models</title><link>https://example.org/docs/models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/docs/models/</guid><description>&lt;h1 id="ollama-model-library">Ollama Model Library&lt;/h1>
&lt;p>Ollama supports a wide variety of open-source large language models (LLMs). This section provides information about the most popular models available for use with Ollama.&lt;/p>
&lt;h2 id="available-models">Available Models&lt;/h2>
&lt;p>Here&amp;rsquo;s an overview of the main models available for Ollama:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>Model&lt;/th>
 &lt;th>Size&lt;/th>
 &lt;th>Strengths&lt;/th>
 &lt;th>Ideal Use Cases&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>Llama 2&lt;/td>
 &lt;td>7B to 70B&lt;/td>
 &lt;td>Well-balanced performance&lt;/td>
 &lt;td>General purpose, chat, coding&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Mistral&lt;/td>
 &lt;td>7B&lt;/td>
 &lt;td>Strong at reasoning&lt;/td>
 &lt;td>Creative writing, complex reasoning&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Gemma&lt;/td>
 &lt;td>2B, 7B&lt;/td>
 &lt;td>Google&amp;rsquo;s lightweight model&lt;/td>
 &lt;td>Simple tasks, embedded systems&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>Phi&lt;/td>
 &lt;td>2B&lt;/td>
 &lt;td>Microsoft&amp;rsquo;s small but capable model&lt;/td>
 &lt;td>Education, lightweight applications&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table></description></item><item><title>API Reference</title><link>https://example.org/docs/api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/docs/api/</guid><description>&lt;h1 id="ollama-api-reference">Ollama API Reference&lt;/h1>
&lt;p>Ollama provides a comprehensive HTTP API that allows you to integrate LLMs into your applications.&lt;/p>
&lt;h2 id="api-base-url">API Base URL&lt;/h2>
&lt;p>The Ollama API is available at:&lt;/p>
&lt;pre tabindex="0">&lt;code>http://localhost:11434/api
&lt;/code>&lt;/pre>&lt;h2 id="chat-endpoint">Chat Endpoint&lt;/h2>
&lt;h3 id="post-apichat">POST /api/chat&lt;/h3>
&lt;p>Start a chat session with a model.&lt;/p>
&lt;h4 id="request">Request&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;model&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;llama2&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;messages&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;role&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;user&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;content&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Hello, how are you?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;stream&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>