---
title: "Advanced Usage"
weight: 4
---

# Advanced Ollama Usage

This section covers advanced topics for getting the most out of Ollama. Whether you're looking to optimize performance, customize models, or integrate Ollama with other tools, you'll find detailed guides here.

## Available Topics

- **[Performance Optimization](/docs/advanced/performance/)**: Techniques for improving model loading time, inference speed, and memory usage
- **[Modelfiles](/docs/advanced/modelfiles/)**: Create custom model configurations
- **[Fine-Tuning](/docs/advanced/fine-tuning/)**: Customize models for specific domains or tasks
- **[GPU Acceleration](/docs/advanced/gpu-acceleration/)**: Configure Ollama to use your GPU for faster inference
- **[Embeddings](/docs/advanced/embeddings/)**: Generate vector embeddings for semantic search and other applications
- **[Multi-Modal Models](/docs/advanced/multi-modal/)**: Work with models that handle both text and images
- **[Langchain Integration](/docs/advanced/langchain-integration/)**: Use Ollama with Langchain for complex AI workflows
- **[Docker Integration](/docs/advanced/docker/)**: Run Ollama in Docker containers
- **[Kubernetes Deployment](/docs/advanced/kubernetes/)**: Deploy Ollama in Kubernetes environments